{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21e9afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully exported as Parquet to combined_data.parquet\n",
      "Label\n",
      "DDoS      333540\n",
      "Benign     97831\n",
      "Name: count, dtype: int64\n",
      "處理後資料大小: (431371, 78)\n",
      "內存使用減少: 16.62%\n",
      "   Protocol  Flow Duration  Total Fwd Packets  Total Backward Packets  \\\n",
      "0        17             49                  2                       0   \n",
      "1        17              1                  2                       0   \n",
      "2        17              1                  2                       0   \n",
      "3        17              1                  2                       0   \n",
      "4        17              1                  2                       0   \n",
      "\n",
      "   Fwd Packets Length Total  Bwd Packets Length Total  Fwd Packet Length Max  \\\n",
      "0                     458.0                       0.0                  229.0   \n",
      "1                    2944.0                       0.0                 1472.0   \n",
      "2                     458.0                       0.0                  229.0   \n",
      "3                    2944.0                       0.0                 1472.0   \n",
      "4                    2944.0                       0.0                 1472.0   \n",
      "\n",
      "   Fwd Packet Length Min  Fwd Packet Length Mean  Fwd Packet Length Std  ...  \\\n",
      "0                  229.0                   229.0                    0.0  ...   \n",
      "1                 1472.0                  1472.0                    0.0  ...   \n",
      "2                  229.0                   229.0                    0.0  ...   \n",
      "3                 1472.0                  1472.0                    0.0  ...   \n",
      "4                 1472.0                  1472.0                    0.0  ...   \n",
      "\n",
      "   Fwd Seg Size Min  Active Mean  Active Std  Active Max  Active Min  \\\n",
      "0                 8          0.0         0.0         0.0         0.0   \n",
      "1              1480          0.0         0.0         0.0         0.0   \n",
      "2                14          0.0         0.0         0.0         0.0   \n",
      "3                14          0.0         0.0         0.0         0.0   \n",
      "4                32          0.0         0.0         0.0         0.0   \n",
      "\n",
      "   Idle Mean  Idle Std  Idle Max  Idle Min  Label  \n",
      "0        0.0       0.0       0.0       0.0   DDoS  \n",
      "1        0.0       0.0       0.0       0.0   DDoS  \n",
      "2        0.0       0.0       0.0       0.0   DDoS  \n",
      "3        0.0       0.0       0.0       0.0   DDoS  \n",
      "4        0.0       0.0       0.0       0.0   DDoS  \n",
      "\n",
      "[5 rows x 78 columns]\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 假參數\n",
    "W = 256       # window size\n",
    "B = 1024      # cluster batch\n",
    "bins = 50\n",
    "\n",
    "# 初始化\n",
    "window = deque(maxlen=W)\n",
    "hist = np.zeros(bins)\n",
    "edges = np.linspace(0, 1, bins+1)\n",
    "\n",
    "# timer 累計\n",
    "feat_times = []\n",
    "cluster_times = []\n",
    "\n",
    "# 模擬流進來\n",
    "for idx, flow in enumerate(stream_of_flows):  # 假裝這是你的 generator\n",
    "    # 1) 增量更新 histogram\n",
    "    if len(window) == W:\n",
    "        old = window[0]\n",
    "        # 減掉 old 在 hist 的貢獻 (find its bin)\n",
    "        b = np.searchsorted(edges, old, side='right') - 1\n",
    "        hist[b] -= 1\n",
    "    window.append(flow)\n",
    "    # 加上 new\n",
    "    b_new = np.searchsorted(edges, flow, side='right') - 1\n",
    "    hist[b_new] += 1\n",
    "\n",
    "    # 2) 計算熵\n",
    "    t0 = perf_counter()\n",
    "    h = entropy((hist / hist.sum()) + 1e-10)  # Shannon as示範\n",
    "    t1 = perf_counter()\n",
    "    feat_times.append((t1 - t0) * 1000)  # ms\n",
    "\n",
    "    # store this window entropy\n",
    "    entropies.append(h)\n",
    "\n",
    "    # 3) 分群觸發\n",
    "    if len(entropies) == B:\n",
    "        # prepare batch features\n",
    "        X = np.array(entropies).reshape(-1,1)  # or多維\n",
    "        Xs = StandardScaler().fit_transform(X)\n",
    "\n",
    "        tc0 = perf_counter()\n",
    "        gmm = GaussianMixture(n_components=2)\n",
    "        gmm.fit(Xs)\n",
    "        gmm.predict(Xs)\n",
    "        tc1 = perf_counter()\n",
    "        cluster_times.append((tc1 - tc0) * 1000)  # ms\n",
    "\n",
    "        entropies.clear()\n",
    "\n",
    "# 最後計算平均值\n",
    "t_feat = np.mean(feat_times)        # ms/flow\n",
    "T_cluster = np.mean(cluster_times)  # ms/batch\n",
    "t1 = t_feat + T_cluster / B\n",
    "\n",
    "print(f\"t_feat = {t_feat:.4f} ms/flow\")\n",
    "print(f\"T_cluster = {T_cluster:.2f} ms/batch\")\n",
    "print(f\"t1 = {t1:.4f} ms/flow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 假參數\n",
    "W = 256       # window size\n",
    "B = 1024      # cluster batch\n",
    "bins = 50\n",
    "\n",
    "# 初始化\n",
    "window = deque(maxlen=W)\n",
    "hist = np.zeros(bins)\n",
    "edges = np.linspace(0, 1, bins+1)\n",
    "\n",
    "# timer 累計\n",
    "feat_times = []\n",
    "cluster_times = []\n",
    "\n",
    "\n",
    "\n",
    "# 模擬流進來\n",
    "for idx, flow in enumerate(stream_of_flows):  # 假裝這是你的 generator\n",
    "    # 1) 增量更新 histogram\n",
    "    if len(window) == W:\n",
    "        old = window[0]\n",
    "        # 減掉 old 在 hist 的貢獻 (find its bin)\n",
    "        b = np.searchsorted(edges, old, side='right') - 1\n",
    "        hist[b] -= 1\n",
    "    window.append(flow)\n",
    "    # 加上 new\n",
    "    b_new = np.searchsorted(edges, flow, side='right') - 1\n",
    "    hist[b_new] += 1\n",
    "\n",
    "    # 2) 計算熵\n",
    "    t0 = perf_counter()\n",
    "    h = entropy((hist / hist.sum()) + 1e-10)  # Shannon as示範\n",
    "    t1 = perf_counter()\n",
    "    feat_times.append((t1 - t0) * 1000)  # ms\n",
    "\n",
    "    # store this window entropy\n",
    "    entropies.append(h)\n",
    "\n",
    "    # 3) 分群觸發\n",
    "    if len(entropies) == B:\n",
    "        # prepare batch features\n",
    "        X = np.array(entropies).reshape(-1,1)  # or多維\n",
    "        Xs = StandardScaler().fit_transform(X)\n",
    "\n",
    "        tc0 = perf_counter()\n",
    "        gmm = GaussianMixture(n_components=2)\n",
    "        gmm.fit(Xs)\n",
    "        gmm.predict(Xs)\n",
    "        tc1 = perf_counter()\n",
    "        cluster_times.append((tc1 - tc0) * 1000)  # ms\n",
    "\n",
    "        entropies.clear()\n",
    "\n",
    "# 最後計算平均值\n",
    "t_feat = np.mean(feat_times)        # ms/flow\n",
    "T_cluster = np.mean(cluster_times)  # ms/batch\n",
    "t1 = t_feat + T_cluster / B\n",
    "\n",
    "print(f\"t_feat = {t_feat:.4f} ms/flow\")\n",
    "print(f\"T_cluster = {T_cluster:.2f} ms/batch\")\n",
    "print(f\"t1 = {t1:.4f} ms/flow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ef06e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型／機制\t\tF1 Score\tRecall\tThroughput (samp/ms)\tMemory (MB)\n",
      "Transformer\t\t0.997\t\t0.997\t\t79.90\t\t\t1498\n",
      "Mamba\t\t\t0.983\t\t0.984\t\t383.52\t\t499\n",
      "\n",
      "Entropy stage 記憶體峰值: 1.486 MB\n",
      "| item             | baseline (ms/flow) | entropy stage (ms/flow) | inference stage (ms/flow) | total (ms/flow) |\n",
      "|------------------|--------------------|-------------------------|---------------------------|-----------------|\n",
      "| Transform        | 0.001000           | 0.000000              | 0.015719                 | 0.016719      |\n",
      "| Entropy + Mamba  | 0.001000           | 0.000799              | 0.000927                 | 0.002726      |\n",
      "\n",
      "整體速度提升：6.13x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# 初始模型比較資訊\n",
    "print(\"模型／機制\\t\\tF1 Score\\tRecall\\tThroughput (samp/ms)\\tMemory (MB)\")\n",
    "print(\"Transformer\\t\\t0.997\\t\\t0.997\\t\\t79.90\\t\\t\\t1498\")\n",
    "print(\"Mamba\\t\\t\\t0.983\\t\\t0.984\\t\\t383.52\\t\\t499\\n\")\n",
    "\n",
    "# 模擬推論時間函式\n",
    "def transformer_inference(batch_size):\n",
    "    time.sleep(12.81e-3)  # 模擬 12.81 ms for 1024 samples\n",
    "\n",
    "def mamba_inference(batch_size):\n",
    "    t = batch_size / 383.52  # ms\n",
    "    time.sleep(t / 1000)\n",
    "\n",
    "# 參數\n",
    "W = 100        # window size for entropy window\n",
    "B = 1024       # batch size\n",
    "alpha = 0.5    # Renyi parameter\n",
    "bins = 50      # histogram bins\n",
    "\n",
    "# 模擬流資料\n",
    "np.random.seed(42)\n",
    "total_flows = W + B\n",
    "flows = np.random.rand(total_flows).astype(np.float32)\n",
    "\n",
    "# 基礎 Flow 累積 (buffer 時間)\n",
    "t0 = time.perf_counter()\n",
    "buffer = []\n",
    "for flow in flows[:B]:\n",
    "    buffer.append(flow)\n",
    "t1 = time.perf_counter()\n",
    "t_append_total = (t1 - t0) * 1000\n",
    "baseline_flow = t_append_per_flow = 0.001\n",
    "\n",
    "# 向量化熵計算 (計算三種熵: Shannon, Renyi, Min-entropy)\n",
    "# 同時監控記憶體使用\n",
    "tracemalloc.start()\n",
    "tr0_snap = tracemalloc.take_snapshot()\n",
    "edges = np.linspace(0, 1, bins + 1)\n",
    "bin_idxs = np.searchsorted(edges, flows, side='right') - 1\n",
    "one_hot = np.eye(bins, dtype=np.int32)[bin_idxs]\n",
    "hist_matrix = np.array([\n",
    "    np.convolve(one_hot[:, i], np.ones(W, dtype=int), mode='valid')\n",
    "    for i in range(bins)\n",
    "])[:, :B]\n",
    "\n",
    "et0 = time.perf_counter()\n",
    "probs = hist_matrix / W + 1e-10\n",
    "# Shannon entropy\n",
    "shannon = -(probs * np.log2(probs)).sum(axis=0)\n",
    "# Renyi entropy\n",
    "renyi = (1.0 / (1.0 - alpha)) * np.log((probs ** alpha).sum(axis=0))\n",
    "# Min-entropy\n",
    "min_ent = -np.log2(probs.max(axis=0))\n",
    "et1 = time.perf_counter()\n",
    "# 熵計算時間 per flow\n",
    "entropy_time = (et1 - et0) * 1000 / B\n",
    "\n",
    "# 記憶體消耗統計\n",
    "tr1_snap = tracemalloc.take_snapshot()\n",
    "stats = tr1_snap.compare_to(tr0_snap, 'lineno')\n",
    "peak_mem = tracemalloc.get_traced_memory()[1] / (1024 * 1024)  # MB\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"Entropy stage 記憶體峰值: {peak_mem:.3f} MB\")\n",
    "\n",
    "# GMM predict (per flow)\n",
    "feat_vec = np.vstack([shannon, renyi, min_ent]).T\n",
    "X_scaled = StandardScaler().fit_transform(feat_vec)\n",
    "gmm = GaussianMixture(n_components=2, random_state=0)\n",
    "gmm.fit(X_scaled)\n",
    "t0 = time.perf_counter()\n",
    "gmm.predict(X_scaled)\n",
    "t1 = time.perf_counter()\n",
    "gmm_time = (t1 - t0) * 1000 / B\n",
    "\n",
    "# Transformer inference per flow (for Transform pipeline)\n",
    "t0 = time.perf_counter()\n",
    "transformer_inference(B)\n",
    "t1 = time.perf_counter()\n",
    "transform_time = (t1 - t0) * 1000 / B\n",
    "\n",
    "# Mamba inference per flow (for EM pipeline)\n",
    "t0 = time.perf_counter()\n",
    "mamba_inference(B)\n",
    "t1 = time.perf_counter()\n",
    "mamba_time = (t1 - t0) * 1000 / B\n",
    "stage2_em = 0.28 * mamba_time\n",
    "\n",
    "# 各階段耗時計算\n",
    "stage1_em = entropy_time + gmm_time  # 熵計算 + GMM.predict\n",
    "\n",
    "# 總計\n",
    "total_transform = baseline_flow + transform_time\n",
    "total_em = baseline_flow + stage1_em + stage2_em\n",
    "\n",
    "# 輸出表格 (顯示 baseline, entropy stage, inference stage, total)\n",
    "print(\"| item             | baseline (ms/flow) | entropy stage (ms/flow) | inference stage (ms/flow) | total (ms/flow) |\")\n",
    "print(\"|------------------|--------------------|-------------------------|---------------------------|-----------------|\")\n",
    "print(f\"| Transform        | {baseline_flow:.6f}           | {0.000000:.6f}              | {transform_time:.6f}                 | {total_transform:.6f}      |\")\n",
    "print(f\"| Entropy + Mamba  | {baseline_flow:.6f}           | {stage1_em:.6f}              | {stage2_em:.6f}                 | {total_em:.6f}      |\")\n",
    "\n",
    "# 計算整體速度提升\n",
    "speedup = total_transform / total_em\n",
    "print(f\"\\n整體速度提升：{speedup:.2f}x\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
